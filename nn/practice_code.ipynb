{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tsensor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "nn = [1, 3, 4, 2, 1]\n",
    "inputs = np.random.randint(10, 100, 10).reshape(10, 1)\n",
    "tagets = [n + np.random.randint(-5, 5) for n in inputs]\n",
    "lr = 1e-5\n",
    "\n",
    "# instantiation ransom weights and biases\n",
    "np.random.seed(42)\n",
    "layers = []\n",
    "for i in range(1, len(nn)):\n",
    "    layers.append([np.random.rand(nn[i - 1], nn[i]) / 5 - 0.1, np.ones((1, nn[i]))])\n",
    "\n",
    "# forward pass\n",
    "l1_output = np.maximum(0, inputs @ layers[0][0] + layers[0][1])  # hidden layer 1 output\n",
    "l2_output = np.maximum(\n",
    "    0, l1_output @ layers[1][0] + layers[1][1]\n",
    ")  # hidden layer 2 output\n",
    "l3_output = np.maximum(\n",
    "    0, l2_output @ layers[2][0] + layers[2][1]\n",
    ")  # hidden layer 3 output\n",
    "output = l3_output @ layers[3][0] + layers[3][1]  # output layers without relu\n",
    "\n",
    "# back propagation and weight and bias updates\n",
    "# from output to layer 4\n",
    "output_gradient = output\n",
    "l4_w_gradient = l3_output.T @ output_gradient\n",
    "l4_b_gradient = np.mean(output_gradient, axis=0)\n",
    "layers[3][0] -= lr * l4_w_gradient\n",
    "layers[3][1] -= lr * l4_b_gradient\n",
    "\n",
    "# from layer4 to layer3\n",
    "l3_gradient = output_gradient @ layers[3][0].T * np.heaviside(l3_output, 0)\n",
    "l3_w_gradient = l2_output.T @ l3_gradient\n",
    "l3_b_gradient = np.mean(l3_gradient, axis=0)\n",
    "layers[2][0] -= lr * l3_w_gradient\n",
    "layers[2][1] -= lr * l3_b_gradient\n",
    "\n",
    "# form layer 3 to 2\n",
    "l2_gradient = l3_gradient @ layers[2][0].T * np.heaviside(l2_output, 0)\n",
    "l2_w_gradient = l1_output.T @ l2_gradient\n",
    "l2_b_gradient = np.mean(l2_gradient, axis=0)\n",
    "layers[1][0] -= lr * l2_w_gradient\n",
    "layers[1][1] -= lr * l2_b_gradient\n",
    "\n",
    "# from 2 to 1\n",
    "l1_gradient = l2_gradient @ layers[1][0].T * np.heaviside(l1_output, 0)\n",
    "l1_w_gradient = inputs.T @ l1_gradient\n",
    "l1_b_gradient = np.mean(l1_gradient, axis=0)\n",
    "layers[0][0] -= lr * l1_w_gradient\n",
    "layers[0][1] -= lr * l1_b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genrating random inputs for temperatures\n",
    "def inputs_targets_genrator(n_instances: int = 10, n_features: int = 1):\n",
    "    inputs = np.random.rand(n_instances, n_features)\n",
    "    valid = np.random.rand(n_instances, n_features)\n",
    "    targets = np.random.rand(n_instances, 1)\n",
    "    valid_y = np.random.rand(n_instances, 1)\n",
    "    return inputs, targets, valid, valid_y\n",
    "\n",
    "\n",
    "# instantiating random weights and biases\n",
    "def weights_biases(nn):\n",
    "    # store the weights\n",
    "    layers = []\n",
    "    for i in range(1, len(nn)):\n",
    "        layers.append([np.random.rand(nn[i - 1], nn[i]) / 5 - 0.1, np.ones((1, nn[i]))])\n",
    "    return layers\n",
    "\n",
    "\n",
    "def forwardpass(layers, batch):\n",
    "    hidden_outputs = [batch.copy()]\n",
    "    for i in range(len(layers)):\n",
    "        batch = batch @ layers[i][0] + layers[i][1]\n",
    "        if i < len(layers) - 1:\n",
    "            np.maximum(batch, 0)\n",
    "        hidden_outputs.append(batch.copy())\n",
    "    return batch, hidden_outputs\n",
    "\n",
    "\n",
    "def backpropagation(layers, hidden_outputs, grad, lr):\n",
    "    for i in range(len(layers) - 1, -1, -1):\n",
    "        if i != len(layers) - 1:\n",
    "            grad = grad * np.heaviside(hidden_outputs[i + 1], 0)\n",
    "\n",
    "        w_grad = hidden_outputs[i].T @ grad\n",
    "        b_grad = np.mean(grad, axis=0)\n",
    "\n",
    "        layers[i][0] -= lr * w_grad\n",
    "        layers[i][1] -= lr * b_grad\n",
    "        # next gradient\n",
    "        grad = grad @ layers[i][0].T\n",
    "    return layers\n",
    "\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual - predicted) ** 2)\n",
    "\n",
    "\n",
    "# Correct MSE gradient function\n",
    "def mse_grad(actual, predicted):\n",
    "    n = actual.shape[0]  # Number of samples\n",
    "    return (2 / n) * (predicted - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train MSE: 0.004989695612552372, Valid MSE: 0.08167866989182765\n",
      "Epoch 2: Train MSE: 0.003283832759902319, Valid MSE: 0.08165543803735191\n",
      "Epoch 3: Train MSE: 0.0032837384409312486, Valid MSE: 0.08165540933646886\n",
      "Epoch 4: Train MSE: 0.003283738431972446, Valid MSE: 0.08165541234353893\n",
      "Epoch 5: Train MSE: 0.003283738555038937, Valid MSE: 0.081655415398315\n",
      "Epoch 6: Train MSE: 0.003283738678331166, Valid MSE: 0.08165541845301028\n",
      "Epoch 7: Train MSE: 0.0032837388016502198, Valid MSE: 0.08165542150755252\n",
      "Epoch 8: Train MSE: 0.003283738924995803, Valid MSE: 0.08165542456194146\n",
      "Epoch 9: Train MSE: 0.0032837390483679224, Valid MSE: 0.08165542761617692\n",
      "Epoch 10: Train MSE: 0.0032837391717665805, Valid MSE: 0.0816554306702587\n",
      "Epoch 11: Train MSE: 0.0032837392951917837, Valid MSE: 0.08165543372418667\n",
      "Epoch 12: Train MSE: 0.0032837394186435364, Valid MSE: 0.08165543677796062\n",
      "Epoch 13: Train MSE: 0.0032837395421218434, Valid MSE: 0.08165543983158041\n",
      "Epoch 14: Train MSE: 0.003283739665626711, Valid MSE: 0.08165544288504584\n",
      "Epoch 15: Train MSE: 0.003283739789158142, Valid MSE: 0.08165544593835677\n",
      "Epoch 16: Train MSE: 0.0032837399127161435, Valid MSE: 0.08165544899151303\n",
      "Epoch 17: Train MSE: 0.0032837400363007196, Valid MSE: 0.08165545204451442\n",
      "Epoch 18: Train MSE: 0.0032837401599118752, Valid MSE: 0.08165545509736077\n",
      "Epoch 19: Train MSE: 0.0032837402835496155, Valid MSE: 0.08165545815005194\n",
      "Epoch 20: Train MSE: 0.0032837404072139456, Valid MSE: 0.08165546120258775\n",
      "Epoch 21: Train MSE: 0.0032837405309048702, Valid MSE: 0.081655464254968\n",
      "Epoch 22: Train MSE: 0.0032837406546223943, Valid MSE: 0.08165546730719256\n",
      "Epoch 23: Train MSE: 0.003283740778366523, Valid MSE: 0.08165547035926124\n",
      "Epoch 24: Train MSE: 0.0032837409021372623, Valid MSE: 0.08165547341117388\n",
      "Epoch 25: Train MSE: 0.0032837410259346153, Valid MSE: 0.08165547646293025\n",
      "Epoch 26: Train MSE: 0.0032837411497585885, Valid MSE: 0.08165547951453027\n",
      "Epoch 27: Train MSE: 0.0032837412736091867, Valid MSE: 0.08165548256597374\n",
      "Epoch 28: Train MSE: 0.003283741397486415, Valid MSE: 0.08165548561726048\n",
      "Epoch 29: Train MSE: 0.0032837415213902775, Valid MSE: 0.0816554886683903\n",
      "Epoch 30: Train MSE: 0.0032837416453207805, Valid MSE: 0.08165549171936304\n",
      "Epoch 31: Train MSE: 0.0032837417692779284, Valid MSE: 0.08165549477017854\n",
      "Epoch 32: Train MSE: 0.003283741893261725, Valid MSE: 0.08165549782083666\n",
      "Epoch 33: Train MSE: 0.0032837420172721786, Valid MSE: 0.08165550087133716\n",
      "Epoch 34: Train MSE: 0.0032837421413092907, Valid MSE: 0.08165550392167994\n",
      "Epoch 35: Train MSE: 0.0032837422653730687, Valid MSE: 0.0816555069718648\n",
      "Epoch 36: Train MSE: 0.0032837423894635158, Valid MSE: 0.08165551002189154\n",
      "Epoch 37: Train MSE: 0.003283742513580639, Valid MSE: 0.08165551307176004\n",
      "Epoch 38: Train MSE: 0.0032837426377244423, Valid MSE: 0.0816555161214701\n",
      "Epoch 39: Train MSE: 0.0032837427618949305, Valid MSE: 0.08165551917102157\n",
      "Epoch 40: Train MSE: 0.003283742886092109, Valid MSE: 0.08165552222041424\n",
      "Epoch 41: Train MSE: 0.0032837430103159833, Valid MSE: 0.08165552526964798\n",
      "Epoch 42: Train MSE: 0.0032837431345665584, Valid MSE: 0.08165552831872262\n",
      "Epoch 43: Train MSE: 0.0032837432588438374, Valid MSE: 0.08165553136763798\n",
      "Epoch 44: Train MSE: 0.0032837433831478286, Valid MSE: 0.08165553441639388\n",
      "Epoch 45: Train MSE: 0.0032837435074785344, Valid MSE: 0.08165553746499016\n",
      "Epoch 46: Train MSE: 0.0032837436318359616, Valid MSE: 0.08165554051342665\n",
      "Epoch 47: Train MSE: 0.0032837437562201143, Valid MSE: 0.0816555435617032\n",
      "Epoch 48: Train MSE: 0.003283743880630998, Valid MSE: 0.08165554660981961\n",
      "Epoch 49: Train MSE: 0.003283744005068617, Valid MSE: 0.08165554965777573\n",
      "Epoch 50: Train MSE: 0.003283744129532977, Valid MSE: 0.08165555270557136\n",
      "Epoch 51: Train MSE: 0.003283744254024084, Valid MSE: 0.08165555575320636\n",
      "Epoch 52: Train MSE: 0.003283744378541941, Valid MSE: 0.08165555880068055\n",
      "Epoch 53: Train MSE: 0.003283744503086556, Valid MSE: 0.08165556184799377\n",
      "Epoch 54: Train MSE: 0.0032837446276579303, Valid MSE: 0.08165556489514586\n",
      "Epoch 55: Train MSE: 0.003283744752256072, Valid MSE: 0.08165556794213662\n",
      "Epoch 56: Train MSE: 0.0032837448768809853, Valid MSE: 0.0816555709889659\n",
      "Epoch 57: Train MSE: 0.003283745001532675, Valid MSE: 0.08165557403563353\n",
      "Epoch 58: Train MSE: 0.003283745126211146, Valid MSE: 0.08165557708213933\n",
      "Epoch 59: Train MSE: 0.0032837452509164038, Valid MSE: 0.08165558012848315\n",
      "Epoch 60: Train MSE: 0.0032837453756484544, Valid MSE: 0.0816555831746648\n",
      "Epoch 61: Train MSE: 0.003283745500407301, Valid MSE: 0.08165558622068414\n",
      "Epoch 62: Train MSE: 0.0032837456251929506, Valid MSE: 0.08165558926654096\n",
      "Epoch 63: Train MSE: 0.0032837457500054065, Valid MSE: 0.08165559231223513\n",
      "Epoch 64: Train MSE: 0.003283745874844675, Valid MSE: 0.08165559535776645\n",
      "Epoch 65: Train MSE: 0.0032837459997107606, Valid MSE: 0.08165559840313476\n",
      "Epoch 66: Train MSE: 0.0032837461246036687, Valid MSE: 0.08165560144833992\n",
      "Epoch 67: Train MSE: 0.003283746249523405, Valid MSE: 0.08165560449338173\n",
      "Epoch 68: Train MSE: 0.003283746374469974, Valid MSE: 0.08165560753826001\n",
      "Epoch 69: Train MSE: 0.003283746499443381, Valid MSE: 0.08165561058297464\n",
      "Epoch 70: Train MSE: 0.0032837466244436293, Valid MSE: 0.08165561362752542\n",
      "Epoch 71: Train MSE: 0.0032837467494707274, Valid MSE: 0.08165561667191217\n",
      "Epoch 72: Train MSE: 0.0032837468745246785, Valid MSE: 0.08165561971613473\n",
      "Epoch 73: Train MSE: 0.003283746999605488, Valid MSE: 0.08165562276019296\n",
      "Epoch 74: Train MSE: 0.0032837471247131613, Valid MSE: 0.08165562580408665\n",
      "Epoch 75: Train MSE: 0.0032837472498477025, Valid MSE: 0.08165562884781566\n",
      "Epoch 76: Train MSE: 0.003283747375009118, Valid MSE: 0.0816556318913798\n",
      "Epoch 77: Train MSE: 0.0032837475001974118, Valid MSE: 0.08165563493477891\n",
      "Epoch 78: Train MSE: 0.0032837476254125903, Valid MSE: 0.08165563797801285\n",
      "Epoch 79: Train MSE: 0.0032837477506546574, Valid MSE: 0.0816556410210814\n",
      "Epoch 80: Train MSE: 0.003283747875923619, Valid MSE: 0.08165564406398444\n",
      "Epoch 81: Train MSE: 0.0032837480012194806, Valid MSE: 0.08165564710672174\n",
      "Epoch 82: Train MSE: 0.0032837481265422467, Valid MSE: 0.0816556501492932\n",
      "Epoch 83: Train MSE: 0.0032837482518919218, Valid MSE: 0.08165565319169864\n",
      "Epoch 84: Train MSE: 0.003283748377268513, Valid MSE: 0.08165565623393783\n",
      "Epoch 85: Train MSE: 0.0032837485026720233, Valid MSE: 0.08165565927601069\n",
      "Epoch 86: Train MSE: 0.0032837486281024593, Valid MSE: 0.08165566231791702\n",
      "Epoch 87: Train MSE: 0.0032837487535598256, Valid MSE: 0.08165566535965658\n",
      "Epoch 88: Train MSE: 0.0032837488790441274, Valid MSE: 0.08165566840122931\n",
      "Epoch 89: Train MSE: 0.0032837490045553707, Valid MSE: 0.081655671442635\n",
      "Epoch 90: Train MSE: 0.003283749130093559, Valid MSE: 0.08165567448387344\n",
      "Epoch 91: Train MSE: 0.003283749255658699, Valid MSE: 0.08165567752494456\n",
      "Epoch 92: Train MSE: 0.0032837493812507947, Valid MSE: 0.08165568056584808\n",
      "Epoch 93: Train MSE: 0.003283749506869852, Valid MSE: 0.0816556836065839\n",
      "Epoch 94: Train MSE: 0.003283749632515876, Valid MSE: 0.08165568664715184\n",
      "Epoch 95: Train MSE: 0.003283749758188872, Valid MSE: 0.08165568968755171\n",
      "Epoch 96: Train MSE: 0.003283749883888844, Valid MSE: 0.0816556927277834\n",
      "Epoch 97: Train MSE: 0.0032837500096157993, Valid MSE: 0.08165569576784669\n",
      "Epoch 98: Train MSE: 0.0032837501353697412, Valid MSE: 0.08165569880774141\n",
      "Epoch 99: Train MSE: 0.0032837502611506755, Valid MSE: 0.08165570184746744\n",
      "Epoch 100: Train MSE: 0.0032837503869586086, Valid MSE: 0.08165570488702455\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "n_instances = 10000\n",
    "n_features = 2\n",
    "layer_config = [n_features, 2, 4, 3, 1]\n",
    "\n",
    "\n",
    "layers = weights_biases(layer_config)\n",
    "\n",
    "inputs, targets, valid, valid_y = inputs_targets_genrator(\n",
    "    n_instances=n_instances, n_features=n_features\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    for i in range(0, inputs.shape[0], batch_size):\n",
    "        train_x = inputs[i : (i + batch_size)]\n",
    "        train_y = targets[i : (i + batch_size)]\n",
    "\n",
    "        prediction, hidden_ouputs = forwardpass(layers, train_x)\n",
    "        loss = mse_grad(train_y, prediction)\n",
    "        epoch_loss.append(np.mean(loss**2))\n",
    "\n",
    "        layers = backpropagation(layers, hidden_ouputs, loss, lr)\n",
    "\n",
    "    valid_pred, _ = forwardpass(layers, valid)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: Train MSE: {np.mean(epoch_loss)}, Valid MSE: {mse(valid_y, valid_pred)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
